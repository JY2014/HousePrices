{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting House Prices using XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.cross_validation import KFold, train_test_split\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read the data\n",
    "x_train = np.loadtxt('data/x_train_cleaned.txt', delimiter=',')\n",
    "x_test = np.loadtxt('data/x_test_cleaned.txt', delimiter=',')\n",
    "y_train_log = np.loadtxt('data/y_train_log.txt', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# formatting for xgb\n",
    "dtrain = xgb.DMatrix(x_train, label=y_train_log)\n",
    "dtest = xgb.DMatrix(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set initial parameters\n",
    "xgb1 = xgb.XGBRegressor(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'reg:linear',\n",
    " nthread = 4,\n",
    " seed=0)\n",
    "\n",
    "xgb_param = xgb1.get_xgb_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Set a large learning rate and tune n_estimators \n",
    "using the default learning rate of 0.1 to tune the other parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model requires 391 estimators.\n",
      "The rmse is  0.1203396\n"
     ]
    }
   ],
   "source": [
    "# use 5-fold CV\n",
    "cv_folds = 5\n",
    "# stop when perfomance does not improve for 50 rounds\n",
    "early_stopping_rounds = 50\n",
    "\n",
    "# tune number of trees\n",
    "cvresult = xgb.cv(xgb_param, dtrain, num_boost_round=xgb_param['n_estimators'], nfold=cv_folds,\n",
    "    metrics='rmse', early_stopping_rounds=early_stopping_rounds)\n",
    "\n",
    "print \"The model requires {} estimators.\". format(cvresult.shape[0])\n",
    "# update the parameter\n",
    "n_estimators = cvresult.shape[0]\n",
    "xgb_param['n_estimators'] = n_estimators\n",
    "# performance\n",
    "print \"The rmse is \", cvresult['test-rmse-mean'][n_estimators-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Function to tune two parameters of XGBoost\n",
    "## Input: name and choices of the two parameters, the model and training data\n",
    "## Output: best score and the two parameters\n",
    "def two_param_tuning (param_name_1, list_1, param_name_2, list_2, xgb_param, dtrain):\n",
    "    best_score = 10e4\n",
    "    best_param_1 = -1\n",
    "    best_param_2 = -1\n",
    "\n",
    "    for param_1 in list_1:\n",
    "        xgb_param[param_name_1] = param_1\n",
    "        \n",
    "        for param_2 in list_2:\n",
    "            xgb_param[param_name_2] = param_2\n",
    "\n",
    "            cvresult = xgb.cv(xgb_param, dtrain, num_boost_round=xgb_param['n_estimators'], nfold=cv_folds,\n",
    "                              metrics='rmse')\n",
    "            score = cvresult['test-rmse-mean'][xgb_param['n_estimators']-1]\n",
    "\n",
    "            if score < best_score:\n",
    "                best_score = score\n",
    "                best_param_1 = param_1\n",
    "                best_param_2 = param_2\n",
    "    \n",
    "    return best_score, best_param_1, best_param_2\n",
    "\n",
    "\n",
    "### Function to tune one parameter of XGBoost\n",
    "## Input: name and choices of the parameter, the model and training data\n",
    "## Output: best score and the parameter\n",
    "def one_param_tuning (param_name_1, list_1, xgb_param, dtrain):\n",
    "    best_score = 10e4\n",
    "    best_param_1 = -1\n",
    "\n",
    "    for param_1 in list_1:\n",
    "        xgb_param[param_name_1] = param_1\n",
    "\n",
    "        cvresult = xgb.cv(xgb_param, dtrain, num_boost_round=xgb_param['n_estimators'], nfold=cv_folds,\n",
    "                          metrics='rmse')\n",
    "        score = cvresult['test-rmse-mean'][xgb_param['n_estimators']-1]\n",
    "\n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "            best_param_1 = param_1\n",
    "    \n",
    "    return best_score, best_param_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tune max_depth and min_child_weight\n",
    "Preventing overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best max_depth is  3\n",
      "The best min_child_weight is  1\n",
      "The rmse is  0.1203396\n"
     ]
    }
   ],
   "source": [
    "# options\n",
    "max_depth_list = range(3, 10, 2)\n",
    "min_child_weight_list = range(1, 6, 2)\n",
    "\n",
    "best_score, best_max_depth, best_min_child_weight = two_param_tuning('max_depth', max_depth_list, \n",
    "                                                                     'min_child_weight', min_child_weight_list,\n",
    "                                                                     xgb_param, dtrain)\n",
    "\n",
    "print \"The best max_depth is \", best_max_depth\n",
    "print \"The best min_child_weight is \", best_min_child_weight\n",
    "print \"The rmse is \", best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best max_depth is  3\n",
      "The best min_child_weight is  1\n",
      "The rmse is  0.1203396\n"
     ]
    }
   ],
   "source": [
    "# fine tune around the best parameter values\n",
    "max_depth_list = range(2, 5, 1)\n",
    "min_child_weight_list = range(1, 3, 1)\n",
    "\n",
    "best_score, best_max_depth, best_min_child_weight = two_param_tuning('max_depth', max_depth_list, \n",
    "                                                                     'min_child_weight', min_child_weight_list,\n",
    "                                                                     xgb_param, dtrain)\n",
    "\n",
    "print \"The best max_depth is \", best_max_depth\n",
    "print \"The best min_child_weight is \", best_min_child_weight\n",
    "print \"The rmse is \", best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# update the parameters\n",
    "xgb_param['max_depth'] = best_max_depth\n",
    "xgb_param['min_child_weight'] = best_min_child_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Tune gamma\n",
    "related to loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best gamma is  0.0\n",
      "The rmse is  0.1203396\n"
     ]
    }
   ],
   "source": [
    "gamma_list = [i/100.0 for i in range(0,5)]\n",
    "\n",
    "best_score, best_gamma = one_param_tuning('gamma', gamma_list, xgb_param, dtrain)\n",
    "\n",
    "print \"The best gamma is \", best_gamma\n",
    "print \"The rmse is \", best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# update the parameter\n",
    "xgb_param['gamma'] = best_gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Tune subsample and colsample_bytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best subsample is  0.8\n",
      "The best colsample_bytree is  0.8\n",
      "The rmse is  0.1203396\n"
     ]
    }
   ],
   "source": [
    "subsample_list = [i/10.0 for i in range(6,10)]\n",
    "colsample_bytree_list = [i/10.0 for i in range(6,10)]\n",
    "\n",
    "best_score, best_subsample, best_colsample_bytree = two_param_tuning('subsample', subsample_list, 'colsample_bytree', colsample_bytree_list,\n",
    "                                              xgb_param, dtrain)\n",
    "\n",
    "print \"The best subsample is \", best_subsample\n",
    "print \"The best colsample_bytree is \", best_colsample_bytree\n",
    "print \"The rmse is \", best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# update the parameters\n",
    "xgb_param['subsample'] = best_subsample\n",
    "xgb_param['colsample_bytree'] = best_colsample_bytree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Fine tuning\n",
    "Using small learning rate and large number of estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model requires 2427 estimators.\n",
      "The rmse is  0.1188416\n"
     ]
    }
   ],
   "source": [
    "xgb_param['learning_rate'] = 0.01\n",
    "xgb_param['n_estimators'] = 5000\n",
    "\n",
    "early_stopping_rounds = 50\n",
    "\n",
    "cvresult = xgb.cv(xgb_param, dtrain, num_boost_round=xgb_param['n_estimators'], nfold=cv_folds,\n",
    "    metrics='rmse', early_stopping_rounds=early_stopping_rounds)\n",
    "\n",
    "print \"The model requires {} estimators.\". format(cvresult.shape[0])\n",
    "# update the parameter\n",
    "n_estimators = cvresult.shape[0]\n",
    "xgb_param['n_estimators'] = n_estimators\n",
    "# performance\n",
    "print \"The rmse is \", cvresult['test-rmse-mean'][n_estimators-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_score': 0.5,\n",
       " 'colsample_bylevel': 1,\n",
       " 'colsample_bytree': 0.8,\n",
       " 'gamma': 0.0,\n",
       " 'learning_rate': 0.01,\n",
       " 'max_delta_step': 0,\n",
       " 'max_depth': 3,\n",
       " 'min_child_weight': 1,\n",
       " 'missing': None,\n",
       " 'n_estimators': 2427,\n",
       " 'nthread': 4,\n",
       " 'objective': 'reg:linear',\n",
       " 'reg_alpha': 0,\n",
       " 'reg_lambda': 1,\n",
       " 'scale_pos_weight': 1,\n",
       " 'seed': 0,\n",
       " 'silent': 1,\n",
       " 'subsample': 0.8}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final parameters\n",
    "xgb_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the final model\n",
    "xgb1 = xgb.XGBRegressor(\n",
    " learning_rate = 0.01,\n",
    " n_estimators = 2427,\n",
    " subsample = 0.8,\n",
    " colsample_bytree = 0.8,\n",
    " objective= 'reg:linear',\n",
    " max_depth = 3,\n",
    " min_child_weight = 1,\n",
    " nthread = 4,\n",
    " seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
       "       gamma=0, learning_rate=0.01, max_delta_step=0, max_depth=3,\n",
       "       min_child_weight=1, missing=None, n_estimators=2427, nthread=4,\n",
       "       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=True, subsample=0.8)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "xgb1.fit(x_train, y_train_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prediction\n",
    "y_pred = xgb1.predict(x_test)\n",
    "\n",
    "# convert to original scale and save\n",
    "submission = pd.read_csv('data/sample_submission.csv')\n",
    "submission['SalePrice'] = np.exp(y_pred)\n",
    "submission.to_csv('results/xgb.csv', sep = ',', index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
