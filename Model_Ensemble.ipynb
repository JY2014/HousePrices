{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge as Ridge_Reg\n",
    "from sklearn.linear_model import Lasso as Lasso_Reg\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import r2_score\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the results from the previous models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the results from all the models\n",
    "train_result = pd.read_csv('results/train_results.csv')\n",
    "test_result = pd.read_csv('results/test_results.csv')\n",
    "train_xgb = pd.read_csv('results/xgb_train.csv', header = None)\n",
    "test_xgb = pd.read_csv('results/xgb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# combine the results for training and testing the ensemble models\n",
    "x_train_df = pd.concat([train_result, train_xgb], axis = 1)\n",
    "x_test_df = pd.concat([test_result, test_xgb[[1]]], axis = 1)\n",
    "# extract the values\n",
    "x_train = x_train_df.values\n",
    "x_test = x_test_df.values\n",
    "# the results from XGboost are converted by log-transformation\n",
    "x_train[:, 3] = np.log(x_train[:, 3])\n",
    "x_test[:, 3] = np.log(x_test[:, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get the y value of the training set\n",
    "y_train_log = np.loadtxt('data/y_train_log.txt', delimiter=',')\n",
    "n = len(y_train_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Ensemble models\n",
    "\n",
    "#### 0. Baseline: average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The baseline RMSLE is  0.12173111405\n"
     ]
    }
   ],
   "source": [
    "y_pred_average = np.mean(x_train, axis = 1)\n",
    "\n",
    "rmse = np.sqrt(np.sum((y_pred_average - y_train_log)**2)/n)\n",
    "print \"The baseline RMSLE is \", rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Lasso regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The R-squared of the best lasso model is  0.910767719631\n",
      "The best alpha is  0.001\n",
      "The model yields RMSLE of  0.119406478031\n"
     ]
    }
   ],
   "source": [
    "## Lasso regression\n",
    "# parameters\n",
    "param = np.power(10.0, range(-4, 5, 1))\n",
    "# tune lasso regression\n",
    "model = Lasso_Reg()\n",
    "grid_model = GridSearchCV(model, param_grid = {'alpha': param}, cv  = 5)\n",
    "grid_model.fit(x_train, y_train_log)\n",
    "# best model\n",
    "lasso = grid_model.best_estimator_\n",
    "print \"The R-squared of the best lasso model is \", grid_model.best_score_\n",
    "print \"The best alpha is \", lasso.get_params()['alpha']\n",
    "\n",
    "## Prediction\n",
    "# training\n",
    "y_pred_train_lasso = cross_val_predict(lasso, x_train, y_train_log, cv = 5)\n",
    "rmse = np.sqrt(np.sum((y_pred_train_lasso - y_train_log)**2)/n)\n",
    "print \"The model yields RMSLE of \", rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The R-squared of the best ridge model is  0.910605293573\n",
      "The best alpha is  0.1\n",
      "The model yield of RMSLE of  0.1195149942\n"
     ]
    }
   ],
   "source": [
    "## Ridge regression\n",
    "# tune lasso regression using the same parameter list\n",
    "model = Ridge_Reg()\n",
    "grid_model = GridSearchCV(model, param_grid = {'alpha': param}, cv  = 5)\n",
    "grid_model.fit(x_train, y_train_log)\n",
    "# best model\n",
    "ridge = grid_model.best_estimator_\n",
    "print \"The R-squared of the best ridge model is \", grid_model.best_score_\n",
    "print \"The best alpha is \", ridge.get_params()['alpha']\n",
    "\n",
    "## Prediction\n",
    "# training\n",
    "y_pred_train_ridge = cross_val_predict(ridge, x_train, y_train_log, cv = 5)\n",
    "rmse = np.sqrt(np.sum((y_pred_train_ridge - y_train_log)**2)/n)\n",
    "print \"The model yield of RMSLE of \", rmse\n",
    "# # testing\n",
    "# ridge.fit(x_train_std, y_train_log)\n",
    "# y_pred_test_ridge = ridge.predict(x_test_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Random Forest Regressor\n",
    "Tune max_depth and min_samples_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model yields RMSLE of  0.121859332638\n"
     ]
    }
   ],
   "source": [
    "## Random Forest Regressor\n",
    "# parameters\n",
    "max_depth = range(1, 8, 1)\n",
    "min_samples_leaf = range(1, 4, 1)\n",
    "\n",
    "# tune RF regressor\n",
    "model = RandomForestRegressor(n_estimators=100)\n",
    "grid_model = GridSearchCV(model, param_grid = {'max_depth': max_depth, 'min_samples_leaf': min_samples_leaf},\n",
    "                          cv  = 5)\n",
    "grid_model.fit(x_train, y_train_log)\n",
    "\n",
    "## Prediction on the training set\n",
    "RF = grid_model.best_estimator_\n",
    "y_pred_train_RF = cross_val_predict(RF, x_train, y_train_log, cv = 5)\n",
    "rmse = np.sqrt(np.sum((y_pred_train_RF - y_train_log)**2)/n)\n",
    "print \"The model yields RMSLE of \", rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# formatting for xgb\n",
    "dtrain = xgb.DMatrix(x_train, label=y_train_log)\n",
    "dtest = xgb.DMatrix(x_test)\n",
    "\n",
    "# set initial parameters\n",
    "xgb1 = xgb.XGBRegressor(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'reg:linear',\n",
    " nthread = 4,\n",
    " seed=0)\n",
    "\n",
    "xgb_param = xgb1.get_xgb_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the default learning rate of 0.2 to tune the other parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model requires 77 estimators.\n",
      "The rmse is  0.1225666\n"
     ]
    }
   ],
   "source": [
    "# use 5-fold CV\n",
    "cv_folds = 5\n",
    "# stop when perfomance does not improve for 50 rounds\n",
    "early_stopping_rounds = 50\n",
    "\n",
    "# tune number of trees\n",
    "cvresult = xgb.cv(xgb_param, dtrain, num_boost_round=xgb_param['n_estimators'], nfold=cv_folds,\n",
    "    metrics='rmse', early_stopping_rounds=early_stopping_rounds)\n",
    "\n",
    "print \"The model requires {} estimators.\". format(cvresult.shape[0])\n",
    "# update the parameter\n",
    "n_estimators = cvresult.shape[0]\n",
    "xgb_param['n_estimators'] = n_estimators\n",
    "# performance\n",
    "print \"The rmse is \", cvresult['test-rmse-mean'][n_estimators-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Function to tune two parameters of XGBoost\n",
    "## Input: name and choices of the two parameters, the model and training data\n",
    "## Output: best score and the two parameters\n",
    "def two_param_tuning (param_name_1, list_1, param_name_2, list_2, xgb_param, dtrain):\n",
    "    best_score = 10e4\n",
    "    best_param_1 = -1\n",
    "    best_param_2 = -1\n",
    "\n",
    "    for param_1 in list_1:\n",
    "        xgb_param[param_name_1] = param_1\n",
    "        \n",
    "        for param_2 in list_2:\n",
    "            xgb_param[param_name_2] = param_2\n",
    "\n",
    "            cvresult = xgb.cv(xgb_param, dtrain, num_boost_round=xgb_param['n_estimators'], nfold=cv_folds,\n",
    "                              metrics='rmse')\n",
    "            score = cvresult['test-rmse-mean'][xgb_param['n_estimators']-1]\n",
    "\n",
    "            if score < best_score:\n",
    "                best_score = score\n",
    "                best_param_1 = param_1\n",
    "                best_param_2 = param_2\n",
    "    \n",
    "    return best_score, best_param_1, best_param_2\n",
    "\n",
    "\n",
    "### Function to tune one parameter of XGBoost\n",
    "## Input: name and choices of the parameter, the model and training data\n",
    "## Output: best score and the parameter\n",
    "def one_param_tuning (param_name_1, list_1, xgb_param, dtrain):\n",
    "    best_score = 10e4\n",
    "    best_param_1 = -1\n",
    "\n",
    "    for param_1 in list_1:\n",
    "        xgb_param[param_name_1] = param_1\n",
    "\n",
    "        cvresult = xgb.cv(xgb_param, dtrain, num_boost_round=xgb_param['n_estimators'], nfold=cv_folds,\n",
    "                          metrics='rmse')\n",
    "        score = cvresult['test-rmse-mean'][xgb_param['n_estimators']-1]\n",
    "\n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "            best_param_1 = param_1\n",
    "    \n",
    "    return best_score, best_param_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters to tune:**\n",
    "- max_depth_list and min_child_weight_list\n",
    "- gamma\n",
    "- subsample and colsample_bytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best max_depth is  3\n",
      "The best min_child_weight is  3\n"
     ]
    }
   ],
   "source": [
    "# options\n",
    "max_depth_list = range(1, 6, 2)\n",
    "min_child_weight_list = range(1, 6, 2)\n",
    "\n",
    "best_score, best_max_depth, best_min_child_weight = two_param_tuning('max_depth', max_depth_list, \n",
    "                                                                     'min_child_weight', min_child_weight_list,\n",
    "                                                                     xgb_param, dtrain)\n",
    "\n",
    "print \"The best max_depth is \", best_max_depth\n",
    "print \"The best min_child_weight is \", best_min_child_weight\n",
    "\n",
    "# update the parameters\n",
    "xgb_param['max_depth'] = best_max_depth\n",
    "xgb_param['min_child_weight'] = best_min_child_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best gamma is  0.0\n"
     ]
    }
   ],
   "source": [
    "gamma_list = [i/100.0 for i in range(0,5)]\n",
    "\n",
    "best_score, best_gamma = one_param_tuning('gamma', gamma_list, xgb_param, dtrain)\n",
    "\n",
    "print \"The best gamma is \", best_gamma\n",
    "\n",
    "# update the parameter\n",
    "xgb_param['gamma'] = best_gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best subsample is  0.7\n",
      "The best colsample_bytree is  0.5\n"
     ]
    }
   ],
   "source": [
    "subsample_list = [i/10.0 for i in range(6,10)]\n",
    "colsample_bytree_list = [i/10.0 for i in range(4,8)]\n",
    "\n",
    "best_score, best_subsample, best_colsample_bytree = two_param_tuning('subsample', subsample_list, 'colsample_bytree', colsample_bytree_list,\n",
    "                                              xgb_param, dtrain)\n",
    "\n",
    "print \"The best subsample is \", best_subsample\n",
    "print \"The best colsample_bytree is \", best_colsample_bytree\n",
    "\n",
    "# update the parameters\n",
    "xgb_param['subsample'] = best_subsample\n",
    "xgb_param['colsample_bytree'] = best_colsample_bytree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model requires 739 estimators.\n",
      "The rmse is  0.1180122\n"
     ]
    }
   ],
   "source": [
    "xgb_param['learning_rate'] = 0.01\n",
    "xgb_param['n_estimators'] = 5000\n",
    "\n",
    "early_stopping_rounds = 50\n",
    "\n",
    "cvresult = xgb.cv(xgb_param, dtrain, num_boost_round=xgb_param['n_estimators'], nfold=cv_folds,\n",
    "    metrics='rmse', early_stopping_rounds=early_stopping_rounds)\n",
    "\n",
    "print \"The model requires {} estimators.\". format(cvresult.shape[0])\n",
    "# update the parameter\n",
    "n_estimators = cvresult.shape[0]\n",
    "xgb_param['n_estimators'] = n_estimators\n",
    "# performance\n",
    "print \"The rmse is \", cvresult['test-rmse-mean'][n_estimators-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Predict on the test set using the best model\n",
    "The models with the lowest RMSLE are lasso and XGBoost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## lasso\n",
    "lasso.fit(x_train, y_train_log)\n",
    "y_pred = lasso.predict(x_test)\n",
    "# clean some outliers\n",
    "# if the predicted value is beyond the previous results +-0.5, \n",
    "# replace by the averaged result\n",
    "y_test_average = np.mean(x_test, axis = 1)\n",
    "outliers = (y_pred < np.min(x_test)-0.5) | (y_pred > np.max(x_test) + 0.5)\n",
    "y_pred[outliers] = y_test_average[outliers] \n",
    "\n",
    "# convert to original scale and save\n",
    "submission = pd.read_csv('data/sample_submission.csv')\n",
    "submission['SalePrice'] = np.exp(y_pred)\n",
    "submission.to_csv('results/ensemble.csv', sep = ',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## XGboost\n",
    "xgb1 = xgb.XGBRegressor(\n",
    " learning_rate = 0.01,\n",
    " n_estimators = 739,\n",
    " subsample = 0.7,\n",
    " colsample_bytree = 0.5,\n",
    " objective= 'reg:linear',\n",
    " max_depth = 3,\n",
    " min_child_weight = 3,\n",
    " nthread = 4,\n",
    " seed=0)\n",
    "\n",
    "# fit the model\n",
    "xgb1.fit(x_train, y_train_log)\n",
    "# prediction\n",
    "y_pred = xgb1.predict(x_test)\n",
    "\n",
    "# convert to original scale and save\n",
    "submission['SalePrice'] = np.exp(y_pred)\n",
    "submission.to_csv('results/ensemble_2.csv', sep = ',', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
